{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c49ac566",
   "metadata": {},
   "source": [
    "## Calculate confusion matrices and performance metrics using pretrained weights, domain adaptation with ensemble classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7c61eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from ecg_utilities.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from ecg_utilities import *\n",
    "\n",
    "import torch.nn.functional as Func\n",
    "\n",
    "from pytorch_sklearn import NeuralNetwork\n",
    "from pytorch_sklearn.callbacks import WeightCheckpoint, Verbose, LossPlot, EarlyStopping, Callback, CallbackInfo\n",
    "from pytorch_sklearn.utils.func_utils import to_safe_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b13db535",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_ids = pd.read_csv(osj(\"..\", \"files\", \"patient_ids.csv\"), header=None).to_numpy().reshape(-1)\n",
    "valid_patients = pd.read_csv(osj(\"..\", \"files\", \"valid_patients.csv\"), header=None).to_numpy().reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9a1d218",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = osj(\"..\", \"dataset_training\", \"dataset_domain_adapted\")\n",
    "TRIO_PATH = osj(\"..\", \"dataset_training\", \"dataset_beat_trios_domain_adapted\")\n",
    "DICT_PATH = osj(\"..\", \"dictionaries\", \"dictionaries_5min_sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08ce93ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_PATH = osj(\"..\", \"pretrained\", \"nets\")\n",
    "SAVE_PATH = osj(\"..\", \"savefolder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56b8f845",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = [-1]\n",
    "batch_sizes = [1024]\n",
    "confidences = [0, *np.linspace(0.5, 1, 51)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b4bc092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_array_of_dict_of_confmat(arr):\n",
    "    return np.stack([np.stack(list(d.values())) for d in arr])\n",
    "\n",
    "def extract_array_of_dict_of_dict_of_confmat(arr):\n",
    "    return np.stack([np.stack([np.stack(list(d2.values())) for d2 in d1.values()]) for d1 in arr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eebc8718",
   "metadata": {},
   "source": [
    "## What is collected?\n",
    "- ### Per repeat:\n",
    "    - #### Confusion matrices per patient (34 in total).\n",
    "    - #### Cumulative confusion matrix (1 in total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "538c97ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34/34 [====================] - 233\u001b[2k\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\u001b[2k\u001b[2k\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\u001b[2k\n",
      "34/34 [====================] - 233\u001b[2k\u001b[2k\n",
      "Wall time: 37.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_patient_cms = []\n",
    "all_cms = []\n",
    "all_weights = []\n",
    "all_confs = []\n",
    "repeats = 10\n",
    "\n",
    "for repeat in range(repeats):\n",
    "    patient_cms = {conf:{} for conf in confidences}\n",
    "    cm = {conf:torch.zeros(2, 2) for conf in confidences}\n",
    "    confs = []\n",
    "    \n",
    "    for i, patient_id in enumerate(valid_patients):\n",
    "        dataset = load_N_channel_dataset(patient_id, DATASET_PATH, TRIO_PATH)\n",
    "        train_X, train_y, train_ids, val_X, val_y, val_ids, test_X, test_y, test_ids = dataset.values()\n",
    "        \n",
    "        # For consulting through error energy.\n",
    "        D, F = load_dictionary(patient_id, DICT_PATH)\n",
    "        D, F = torch.Tensor(D), torch.Tensor(F)\n",
    "\n",
    "        ## Consulting Exponential - Gaussian.\n",
    "        BF = BayesianFit()\n",
    "        EF = ExponentialFit()\n",
    "        GF = GaussianFit()\n",
    "\n",
    "        # Train error.\n",
    "        train_E, E_healthy, E_arrhyth = get_error_one_patient(train_X[:, 0, :].squeeze(), F, y=train_y, as_energy=True)\n",
    "        # _, E_healthy, E_arrhyth = get_error_per_patient(train_X[:, 0, :].squeeze(), ids=train_ids, DICT_PATH=DICT_PATH, y=train_y, as_energy=True)\n",
    "        \n",
    "        EF.fit(E_healthy)\n",
    "        GF.fit(E_arrhyth)\n",
    "        consult_train_y = torch.Tensor(BF.predict(train_E, EF, GF) <= 0.5).long()\n",
    "        \n",
    "        # Test Error (be careful, we check (<= 0.5) because EF => healthy => label 0)\n",
    "        test_E = get_error_one_patient(test_X[:, 0, :].squeeze(), F, as_energy=True)\n",
    "        \n",
    "        EF.fit(E_healthy)\n",
    "        GF.fit(E_arrhyth)\n",
    "        consult_test_y = torch.Tensor(BF.predict(test_E, EF, GF) <= 0.5).long()\n",
    "        ##\n",
    "\n",
    "        # Load the neural network.\n",
    "        model = get_base_model(in_channels=train_X.shape[1])\n",
    "        model = model.to(\"cuda\")\n",
    "        crit = nn.CrossEntropyLoss()\n",
    "        optim = torch.optim.AdamW(params=model.parameters())\n",
    "        \n",
    "        net = NeuralNetwork.load_class(osj(LOAD_PATH, f\"net_{repeat+1}_{patient_id}\"), model, optim, crit)\n",
    "        weight_checkpoint_val_loss = net.cbmanager.callbacks[1]  # <- this needs to change in case weight checkpoint is not the second callback.\n",
    "        \n",
    "        net.load_weights(weight_checkpoint_val_loss)\n",
    "        \n",
    "        # Test predictions and probabilities.\n",
    "        pred_y = net.predict(test_X, batch_size=1024, use_cuda=True, fits_gpu=True, decision_func=lambda pred_y: pred_y.argmax(dim=1)).cpu()\n",
    "        prob_y = net.predict_proba(test_X).cpu()\n",
    "        softmax_prob_y = Func.softmax(prob_y, dim=1).max(dim=1).values\n",
    "        \n",
    "        for conf in confidences:\n",
    "            low_confidence = softmax_prob_y < conf\n",
    "            high_confidence = softmax_prob_y >= conf\n",
    "\n",
    "            final_pred_y = torch.Tensor(np.select([low_confidence, high_confidence], [consult_test_y, pred_y])).long()\n",
    "            cm_exp = get_confusion_matrix(final_pred_y, test_y, pos_is_zero=False)\n",
    "\n",
    "            patient_cms[conf][patient_id] = cm_exp\n",
    "            cm[conf] += cm_exp\n",
    "            \n",
    "        print_progress(i + 1, len(valid_patients), opt=[f\"{patient_id}\"])\n",
    "        \n",
    "    all_patient_cms.append(patient_cms)\n",
    "    all_cms.append(cm)\n",
    "    all_confs.append(confs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e735d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = dict(\n",
    "    learning_rate=0.001,\n",
    "    max_epochs=max_epochs[0],\n",
    "    batch_size=batch_sizes[0],\n",
    "    optimizer=optim.__class__.__name__,\n",
    "    loss=crit.__class__.__name__,\n",
    "    early_stopping=\"true\",\n",
    "    checkpoint_on=weight_checkpoint_val_loss.tracked,\n",
    "    dataset=\"default+trio\",\n",
    "    info=\"Results replicated for GitHub, DA + Ensemble + All C.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a0b0f00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cms = extract_array_of_dict_of_confmat(all_cms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59f16d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9775077262350328,\n",
       " 'rec': 0.9068441542132362,\n",
       " 'spe': 0.9874796200467852,\n",
       " 'pre': 0.9108820969511965,\n",
       " 'npv': 0.9868622627441187,\n",
       " 'f1': 0.9088586405885203}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_performance_metrics(all_cms.sum(axis=0)[0])  # only DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "47b4ca5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9466105511639825,\n",
       " 'rec': 0.7938339821675248,\n",
       " 'spe': 0.968170057418303,\n",
       " 'pre': 0.7787345701825708,\n",
       " 'npv': 0.9708264621421057,\n",
       " 'f1': 0.7862117857764732}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_performance_metrics(all_cms.sum(axis=0)[-1]) # only NPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7dd69a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acc': 0.9806684833176107,\n",
       " 'rec': 0.9262730126836619,\n",
       " 'spe': 0.9883446548962487,\n",
       " 'pre': 0.918132842063334,\n",
       " 'npv': 0.9895827729220232,\n",
       " 'f1': 0.9221849643049147}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_performance_metrics(all_cms[:, 1:-1, :, :].sum(axis=(0, 1))) # avg over quantized C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3bad338",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    with open(osj(SAVE_PATH, \"cms.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(all_cms, f)\n",
    "        \n",
    "    with open(osj(SAVE_PATH, \"config.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(config, f)\n",
    "        \n",
    "    with open(osj(SAVE_PATH, \"patient_cms.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(all_patient_cms, f)\n",
    "        \n",
    "    with open(osj(SAVE_PATH, \"confidences.pkl\"), \"wb\") as f:\n",
    "        pickle.dump(all_confs, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
